{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fff769-078f-481d-9826-377cc06bc158",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "Missing values in a dataset refer to the absence of data for certain observations or variables. \n",
    "These missing values can occur for various reasons, such as errors in data collection, data entry issues, or intentional omissions. \n",
    "Handling missing values is essential in data analysis and machine learning for several reasons:\n",
    "Biased Analysis: \n",
    "    If not handled properly, missing values can lead to biased or inaccurate analyses, as the available data may not represent the true characteristics of the population.\n",
    "Reduced Accuracy: \n",
    "    Many machine learning algorithms cannot handle missing values, and attempting to use them without addressing missing data may result in reduced model accuracy.\n",
    "Model Performance: \n",
    "    Missing values can affect the performance of machine learning models, leading to suboptimal predictions or biased results.\n",
    "Data Integrity: \n",
    "    Handling missing values is crucial for maintaining the integrity of the dataset, ensuring that the results are reliable and trustworthy.\n",
    "Statistical Analysis: \n",
    "    Missing values can impact summary statistics, correlations, and other statistical measures, influencing the interpretation of the data.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "Tree-based algorithms: \n",
    "    Decision trees, Random Forest, and Gradient Boosting machines are generally robust to missing values. These algorithms can work well even if some features have missing data.\n",
    "k-Nearest Neighbors (k-NN): \n",
    "    The k-NN algorithm can handle missing values, as it relies on the similarity between data points rather than the absolute values of the features.\n",
    "Naive Bayes: \n",
    "    Naive Bayes is a probabilistic algorithm that can handle missing values gracefully. It makes assumptions about independence between features, which allows it to estimate probabilities even with missing data.\n",
    "Principal Component Analysis (PCA): \n",
    "    PCA is a dimensionality reduction technique that can handle missing values during the calculation of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e63572-f955-425e-81e8-e99ff7874ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64af9ef-8c1e-4922-992b-fffe33033058",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Dropping Missing Values:\n",
    "   This involves removing rows or columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed27a488-2f8e-4c6e-bf44-193bafa125c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with dropped rows:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame with dropped columns:\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_dropped_rows = df.dropna()\n",
    "\n",
    "# Drop columns with missing values\n",
    "df_dropped_cols = df.dropna(axis=1)\n",
    "\n",
    "print(\"DataFrame with dropped rows:\\n\", df_dropped_rows)\n",
    "print(\"\\nDataFrame with dropped columns:\\n\", df_dropped_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607e48a-ff11-4c26-b606-6c76d9ecf4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.Imputation (Filling Missing Values):\n",
    "   Replace missing values with a specified value (e.g., mean, median, or mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaccf8f8-09ab-460e-98f1-2d794cd55f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with mean imputation:\n",
      "           A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n",
      "\n",
      "DataFrame with median imputation:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  7.0\n",
      "2  2.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with mean\n",
    "df_imputed_mean = df.fillna(df.mean())\n",
    "\n",
    "# Impute missing values with median\n",
    "df_imputed_median = df.fillna(df.median())\n",
    "\n",
    "print(\"DataFrame with mean imputation:\\n\", df_imputed_mean)\n",
    "print(\"\\nDataFrame with median imputation:\\n\", df_imputed_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5eab6-a8a4-465d-b553-3f6a03608e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.Forward Fill (ffill) or Backward Fill (bfill):\n",
    "   Propagate the last observed non-null value forward or use the next observed non-null value backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1399cf-c3f2-4fa7-8599-304b6fa038a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with forward fill:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  5.0\n",
      "2  2.0  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame with backward fill:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  7.0\n",
      "2  4.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# Forward fill missing values\n",
    "df_forward_filled = df.ffill()\n",
    "\n",
    "# Backward fill missing values\n",
    "df_backward_filled = df.bfill()\n",
    "\n",
    "print(\"DataFrame with forward fill:\\n\", df_forward_filled)\n",
    "print(\"\\nDataFrame with backward fill:\\n\", df_backward_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa15b96-ff92-4112-8db3-262807b80abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.Interpolation:\n",
    "  Estimate missing values based on the values of other data points using various interpolation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d90bc22-0486-4db7-a5f8-131add4263b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with linear interpolation:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  8.0\n",
      "\n",
      "DataFrame with polynomial interpolation:\n",
      "      A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "# Linear interpolation\n",
    "df_interpolated_linear = df.interpolate(method='linear')\n",
    "\n",
    "# Polynomial interpolation of degree 2\n",
    "df_interpolated_poly = df.interpolate(method='polynomial', order=2)\n",
    "\n",
    "print(\"DataFrame with linear interpolation:\\n\", df_interpolated_linear)\n",
    "print(\"\\nDataFrame with polynomial interpolation:\\n\", df_interpolated_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42876d66-5827-42f3-9e62-f3e075c0acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "\n",
    "Imbalanced data refers to a situation in a classification problem where the distribution of the target classes is not uniform, meaning that one class has significantly fewer instances than the others. \n",
    "In a binary classification scenario, one class is considered the minority class, and the other is the majority class. In multi-class problems, multiple classes may exhibit imbalances.\n",
    "\n",
    "If imbalanced data is not handled, several issues can arise:\n",
    "1.Biased Model Performance: \n",
    "    Machine learning models trained on imbalanced data tend to be biased towards the majority class. The model may perform well in terms of accuracy but may struggle to correctly predict instances of the minority class.\n",
    "2.Poor Generalization: \n",
    "    Imbalanced data can lead to poor generalization of the model to new, unseen data, especially for the minority class. The model may not have learned enough from the minority class examples to make accurate predictions on similar instances in the future.\n",
    "3.Misleading Evaluation Metrics: \n",
    "    Accuracy is not a reliable metric for imbalanced datasets. A model that predicts the majority class for all instances could still achieve high accuracy, but it would be practically useless for the minority class. Evaluation metrics like precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC) are more informative in such cases.\n",
    "4.Model Sensitivity to Class Distribution Changes: \n",
    "    Imbalanced datasets can make machine learning models sensitive to changes in the class distribution. Minor changes in the data or the introduction of new instances may disproportionately affect the model's performance, particularly for the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4f9b2-dcf2-4f83-8708-db8703d6a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "\n",
    "Up-sampling and down-sampling are two common techniques used to address imbalanced datasets by adjusting the class distribution. These techniques are often applied in the context of binary classification, where one class is the minority class (positive class) and the other is the majority class (negative class).\n",
    "1.Up-sampling:\n",
    "   Up-sampling involves increasing the number of instances in the minority class by either replicating existing instances or generating synthetic examples.\n",
    "Example: Fraud Detection\n",
    "   In a credit card fraud detection scenario, fraudulent transactions are typically rare compared to legitimate transactions. \n",
    "   To up-sample the minority class (fraudulent transactions), instances of fraudulent transactions can be replicated, or synthetic instances can be generated using techniques like SMOTE. \n",
    "   This increases the representation of the minority class in the dataset, allowing the model to learn more about the characteristics of fraudulent transactions.\n",
    "    \n",
    "2.Down-sampling:\n",
    "   Down-sampling involves reducing the number of instances in the majority class by randomly removing instances or selecting a subset of instances.\n",
    "Example: Medical Diagnosis\n",
    "   Consider a medical diagnosis scenario where detecting a rare disease is the target. \n",
    "   If the dataset has an overwhelming number of healthy (negative) instances compared to diseased (positive) instances, down-sampling the majority class may be necessary. \n",
    "   This ensures that the model does not become overly biased towards predicting the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106d863-f5ef-48f6-b367-cf12e9a00f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Data Augmentation:\n",
    "  Data augmentation is a technique used to artificially increase the size of a dataset by applying various transformations to the existing data, thereby creating new instances. \n",
    "  This approach is commonly used in image and text data, but it can be adapted to other types of data as well. \n",
    "  Data augmentation helps improve the generalization and robustness of machine learning models by exposing them to a more diverse set of examples.\n",
    "\n",
    "SMOTE:\n",
    "  SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique designed to address imbalanced datasets, particularly in the context of binary classification. SMOTE works by generating synthetic instances for the minority class to balance the class distribution.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "Identify Minority Class Instances: \n",
    "    SMOTE first identifies instances belonging to the minority class.\n",
    "Select Nearest Neighbors: \n",
    "    For each minority class instance, SMOTE selects a specified number of its nearest neighbors. The number of neighbors to be considered is a user-defined parameter.\n",
    "Generate Synthetic Instances: \n",
    "    For each selected minority class instance, SMOTE creates synthetic instances along the line segments connecting it to its nearest neighbors. The number of synthetic instances generated is determined by another user-defined parameter.\n",
    "Combine Original and Synthetic Instances: \n",
    "    The synthetic instances are added to the original dataset, resulting in an augmented dataset with a more balanced class distribution.\n",
    "\n",
    "SMOTE helps prevent the model from being biased toward the majority class and improves its ability to generalize to minority class instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a163c3-b480-4083-947c-d57aa68eb86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "Outliers:\n",
    "  Outliers are data points that significantly differ from the rest of the observations in a dataset. \n",
    "  These data points are often unusually high or low compared to the majority of the data and can have a significant impact on statistical analyses and machine learning models. \n",
    "  Outliers can arise due to various reasons, including measurement errors, experimental error, or the presence of rare events.\n",
    "Handling outliers is essential for several reasons:\n",
    "Impact on Descriptive Statistics: \n",
    "    Outliers can significantly impact summary statistics such as the mean and standard deviation, leading to inaccurate representations of the central tendency and variability of the data.\n",
    "Effect on Machine Learning Models: \n",
    "    Outliers can disproportionately influence the training of machine learning models, leading to models that are skewed or biased. Models such as linear regression can be particularly sensitive to outliers.\n",
    "Distorted Visualizations: \n",
    "    Outliers can distort visualizations, making it challenging to interpret patterns and trends in the data.\n",
    "Violation of Assumptions: \n",
    "    Some statistical methods and machine learning algorithms assume that the data is normally distributed or follows a specific distribution. Outliers can violate these assumptions and lead to unreliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a36e66-b708-4b66-9719-fe27fe189324",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "\n",
    "Handling missing data is a critical step in data analysis to ensure that the insights and models derived from the data are accurate and reliable. \n",
    "Here are some common techniques to handle missing data:\n",
    "\n",
    "1.Dropping Missing Values:\n",
    "  Remove rows or columns containing missing values. \n",
    "  This approach is suitable when the missing data is relatively small in proportion to the total dataset.\n",
    "2.Imputation:\n",
    "  Replace missing values with estimated or calculated values. \n",
    "  Common imputation methods include mean, median, or mode imputation.\n",
    "3.Forward Fill (ffill) or Backward Fill (bfill):\n",
    "  Propagate the last observed non-null value forward or use the next observed non-null value backward.\n",
    "4.Interpolation:\n",
    "  Estimate missing values based on the values of other data points using interpolation methods such as linear or polynomial interpolation.\n",
    "5.Using Machine Learning Models for Imputation:\n",
    "  Train machine learning models to predict missing values based on the observed data. \n",
    "  This approach can be useful when the missing data is not completely at random.\n",
    "6.Marking and Treating as a Separate Category:\n",
    "  In categorical variables, you can create a new category for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153d6c1-a890-451b-8dd8-d6041024ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "Determining whether missing data is missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR) is essential for understanding the nature of the missingness and selecting appropriate strategies for handling it. \n",
    "Here are some strategies to help assess patterns in the missing data:\n",
    "\n",
    "1.Visual Exploration:\n",
    "  Create visualizations, such as heatmaps or missing data matrices, to visualize the distribution of missing values across variables. \n",
    "  This can help identify patterns and relationships between missing values.\n",
    "2.Correlation Analysis:\n",
    "  Examine correlations between missing values in different variables. \n",
    "  A correlation matrix can help identify whether the presence or absence of missing values in one variable is related to another.\n",
    "3.Missing Data Summary Statistics:\n",
    "  Calculate summary statistics separately for observations with missing values and those without. \n",
    "  Compare the distributions to identify if there are systematic differences between the two groups.\n",
    "4.Missingness Tests:\n",
    "  Use statistical tests to check if the missingness pattern is random or follows a specific pattern. \n",
    "  The chi-square test or Fisher's exact test can be applied to categorical variables, and t-tests or ANOVA can be used for continuous variables.\n",
    "5.Time-Based Analysis:\n",
    "  If the dataset has a temporal component, examine whether missing values occur at specific time points or in certain temporal patterns. \n",
    "  This can help identify trends or seasonality in missingness.\n",
    "6.Domain Knowledge and Expert Input:\n",
    "  Consult domain experts or individuals with knowledge about the data generation process to gain insights into why certain values might be missing. \n",
    "  Their input can help interpret patterns in the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587915b-a212-4eb5-b5c9-b7d2ce71b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "Dealing with imbalanced datasets in a medical diagnosis project, where the majority of patients are healthy and only a small percentage have the condition of interest, requires careful consideration to ensure the model's performance is adequately assessed. Here are some strategies to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "1.Use Appropriate Evaluation Metrics:\n",
    "  Avoid relying solely on accuracy, as it can be misleading in imbalanced datasets. \n",
    "  Instead, focus on metrics that provide a more comprehensive assessment, such as precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC).\n",
    "2.Confusion Matrix Analysis:\n",
    "  Analyze the confusion matrix to understand the distribution of true positives, false positives, true negatives, and false negatives. \n",
    "  This can provide insights into where the model is making errors.\n",
    "3.Precision-Recall Curve:\n",
    "  Plot a precision-recall curve to visualize the trade-off between precision and recall at different probability thresholds.\n",
    "4.Adjust Probability Threshold:\n",
    "  Experiment with adjusting the probability threshold for classification to achieve a better balance between precision and recall based on the specific needs of the application.\n",
    "5.Resampling Techniques:\n",
    "  Explore resampling techniques such as oversampling the minority class (patients with the condition) or undersampling the majority class (healthy patients) to balance the class distribution.\n",
    "6.Cost-sensitive Learning:\n",
    "  Assign different misclassification costs to different classes to guide the model to pay more attention to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97dc2b-9d31-49a8-b837-d06e830f8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "When dealing with an imbalanced dataset in which the majority of customers report being satisfied, down-sampling the majority class is a common approach to balance the class distribution. \n",
    "Down-sampling involves reducing the number of instances in the majority class to match the number of instances in the minority class. \n",
    "Here are some methods you can employ to down-sample the majority class:\n",
    "\n",
    "1.Random Under-sampling:\n",
    "  Randomly remove instances from the majority class until the desired balance is achieved.\n",
    "2.Near Miss Algorithm:\n",
    "  Near Miss is an under-sampling technique that selects instances from the majority class such that the average distance to the k-nearest instances in the minority class is minimized.\n",
    "3.Tomek Links:\n",
    "  Tomek Links are pairs of instances, one from the majority class and one from the minority class, that are close to each other. \n",
    "  Removing the majority class instances from these pairs can help down-sample the majority class.\n",
    "4.Edited Nearest Neighbors (ENN):\n",
    "  ENN is an under-sampling technique that removes instances from the majority class if their class label differs from the majority class label of their k-nearest neighbors.\n",
    "5.Combination of Over-sampling and Under-sampling:\n",
    "  A combination of over-sampling the minority class and under-sampling the majority class can sometimes provide a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95b064-02cb-44df-8c5f-b87643e54816",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "When dealing with an imbalanced dataset where the occurrence of a rare event is low, up-sampling the minority class is a common approach to balance the class distribution. \n",
    "Up-sampling involves increasing the number of instances in the minority class to match the number of instances in the majority class. \n",
    "Here are some methods you can employ to up-sample the minority class:\n",
    "\n",
    "1.Random Over-sampling:\n",
    "  Randomly replicate instances from the minority class until the desired balance is achieved.\n",
    "2.SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "  SMOTE generates synthetic instances for the minority class by interpolating between existing instances. \n",
    "  It creates synthetic data points along the line segments connecting minority class instances.\n",
    "3.ADASYN (Adaptive Synthetic Sampling):\n",
    "  ADASYN is an extension of SMOTE that introduces a level of adaptability to generate more synthetic examples for the minority class instances that are harder to learn.\n",
    "4.Random Over-sampling with Replacement:\n",
    "  Randomly select instances from the minority class with replacement until the desired balance is achieved.\n",
    "5.SMOTE-ENN (SMOTE with Edited Nearest Neighbors):\n",
    "  Combine over-sampling using SMOTE with under-sampling using Edited Nearest Neighbors to create a balanced dataset.\n",
    "6.Using Weighted Models:\n",
    "  Assign different misclassification costs to different classes in your model to give more weight to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a2e46-f9be-4a6b-92e8-079c6a871495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
